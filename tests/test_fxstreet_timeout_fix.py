#!/usr/bin/env python3
"""
Test script for FXStreet timeout fix.
Tests the enhanced Crawl4AI extractor with progressive timeout handling.
"""
import os
import sys
import asyncio
from dotenv import load_dotenv
from loguru import logger

# Add project root to path
sys.path.insert(0, os.path.dirname(__file__))

# Load environment variables
load_dotenv()

def test_enhanced_extractor():
    """Test the enhanced Crawl4AI extractor with FXStreet."""
    logger.info("üß™ Testing Enhanced Crawl4AI Extractor for FXStreet timeout fix")
    
    try:
        from crawler.extractors.crawl4ai_extractor import EnhancedCrawl4AIExtractor
        from crawler.interfaces import SourceConfig, SourceType, ContentType
        
        # Create FXStreet configuration
        fxstreet_config = SourceConfig(
            name="fxstreet",
            source_type=SourceType.RSS,
            content_type=ContentType.FOREX,
            base_url="https://www.fxstreet.com",
            rss_url="https://www.fxstreet.com/rss/news",
            rate_limit_seconds=2,
            max_articles_per_run=3,  # Small number for testing
            timeout_seconds=120
        )
        
        logger.info("‚úÖ Successfully imported Enhanced Crawl4AI Extractor")
        logger.info(f"üìã Testing with config: {fxstreet_config.name} - timeout: {fxstreet_config.timeout_seconds}s")
        
        return True
        
    except ImportError as e:
        logger.error(f"‚ùå Import failed: {e}")
        return False
    except Exception as e:
        logger.error(f"‚ùå Configuration error: {e}")
        return False

async def test_fxstreet_article_extraction():
    """Test extracting a specific FXStreet article."""
    try:
        from crawler.extractors.crawl4ai_extractor import EnhancedCrawl4AIExtractor
        from crawler.interfaces import SourceConfig, SourceType, ContentType
        
        # Create FXStreet configuration
        config = SourceConfig(
            name="fxstreet_test",
            source_type=SourceType.RSS,
            content_type=ContentType.FOREX,
            base_url="https://www.fxstreet.com",
            timeout_seconds=120
        )
        
        # Create extractor
        extractor = EnhancedCrawl4AIExtractor(config)
        
        # Test URLs - use recent FXStreet articles
        test_urls = [
            "https://www.fxstreet.com/news/usd-jpy-attempts-recovery-above-14700-ahead-of-jolts-data-20241003",
            "https://www.fxstreet.com/news/eur-usd-holds-above-11050-ecb-nagel-says-more-rate-cuts-likely-20241003"
        ]
        
        for test_url in test_urls:
            try:
                logger.info(f"üîÑ Testing article extraction: {test_url}")
                
                # Test individual article extraction with timeout handling
                article = await extractor.extract_article_content(test_url)
                
                if article:
                    logger.success(f"‚úÖ Successfully extracted article:")
                    logger.info(f"   üì∞ Title: {article.title}")
                    logger.info(f"   üîó URL: {article.url}")
                    logger.info(f"   üìÖ Date: {article.published_date}")
                    logger.info(f"   üìä Source: {article.source_name}")
                else:
                    logger.warning(f"‚ö†Ô∏è No article extracted from {test_url}")
                    
            except Exception as e:
                logger.error(f"‚ùå Failed to extract {test_url}: {str(e)}")
                continue
        
        # Test health check
        logger.info("üîç Testing health check...")
        is_healthy = await extractor.health_check()
        logger.info(f"üíö Health check: {'PASSED' if is_healthy else 'FAILED'}")
        
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Article extraction test failed: {str(e)}")
        return False

async def test_timeout_handling():
    """Test timeout handling specifically."""
    try:
        from crawler.extractors.crawl4ai_extractor import EnhancedCrawl4AIExtractor
        from crawler.interfaces import SourceConfig, SourceType, ContentType
        
        logger.info("‚è±Ô∏è Testing timeout handling mechanisms...")
        
        # Create configuration with short timeout to test timeout handling
        config = SourceConfig(
            name="timeout_test",
            source_type=SourceType.RSS,
            content_type=ContentType.FOREX,
            base_url="https://httpbin.org",  # Reliable test service
            timeout_seconds=5  # Very short timeout
        )
        
        extractor = EnhancedCrawl4AIExtractor(config)
        
        # Test with a URL that should work quickly
        quick_url = "https://httpbin.org/html"
        logger.info(f"üöÄ Testing quick URL: {quick_url}")
        
        article = await extractor.extract_article_content(quick_url)
        
        if article:
            logger.success("‚úÖ Quick URL extraction successful")
            logger.info(f"   üì∞ Title: {article.title}")
        else:
            logger.warning("‚ö†Ô∏è Quick URL extraction returned no article")
        
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Timeout handling test failed: {str(e)}")
        return False

async def main():
    """Main test function."""
    logger.info("üöÄ Starting FXStreet Timeout Fix Tests")
    logger.info("=" * 60)
    
    # Test 1: Basic import and configuration
    logger.info("\nüìã Test 1: Enhanced Extractor Import")
    if not test_enhanced_extractor():
        logger.error("‚ùå Basic import test failed")
        return False
    
    # Test 2: Timeout handling
    logger.info("\n‚è±Ô∏è Test 2: Timeout Handling")
    if not await test_timeout_handling():
        logger.error("‚ùå Timeout handling test failed")
        return False
    
    # Test 3: FXStreet article extraction (this may take time)
    logger.info("\nüì∞ Test 3: FXStreet Article Extraction")
    logger.warning("‚ö†Ô∏è This test may take up to 2 minutes due to FXStreet's heavy site...")
    
    if not await test_fxstreet_article_extraction():
        logger.error("‚ùå FXStreet extraction test failed")
        return False
    
    logger.success("\nüéâ All tests passed! Enhanced Crawl4AI Extractor is working correctly.")
    logger.info("\nüìã Key improvements implemented:")
    logger.info("   ‚úÖ Progressive timeout handling (30s ‚Üí 60s ‚Üí 120s)")
    logger.info("   ‚úÖ Anti-bot detection countermeasures")
    logger.info("   ‚úÖ Enhanced error handling and retries")
    logger.info("   ‚úÖ Optimized browser configuration")
    logger.info("   ‚úÖ Better content extraction fallbacks")
    
    return True

if __name__ == "__main__":
    try:
        result = asyncio.run(main())
        if result:
            logger.info("\n‚úÖ FXStreet timeout fix is ready for production!")
            sys.exit(0)
        else:
            logger.error("\n‚ùå Some tests failed. Check the logs above.")
            sys.exit(1)
    except KeyboardInterrupt:
        logger.info("\n‚ö†Ô∏è Tests interrupted by user")
        sys.exit(1)
    except Exception as e:
        logger.error(f"\nüí• Unexpected error: {str(e)}")
        sys.exit(1)
