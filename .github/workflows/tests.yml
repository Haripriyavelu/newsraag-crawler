name: NewsRaag Crawler - Enhanced Test Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]
  workflow_dispatch:
    inputs:
      test_type:
        description: 'Type of tests to run'
        required: false
        default: 'all'
        type: choice
        options:
          - all
          - unit
          - integration
          - smoke
          - fast

jobs:
  # Quick smoke tests for immediate feedback
  smoke-tests:
    name: üî• Smoke Tests
    runs-on: ubuntu-latest
    if: github.event_name == 'push' || github.event.inputs.test_type == 'smoke' || github.event.inputs.test_type == 'all'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python 3.12
      uses: actions/setup-python@v5
      with:
        python-version: '3.12'

    - name: Cache dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-smoke-${{ hashFiles('**/requirements*.txt') }}

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install pytest pytest-asyncio pytest-mock
        pip install -r requirements.txt

    - name: Run smoke tests
      run: |
        python -m pytest tests/test_basic_setup.py --tb=short --no-cov -v --maxfail=3
      env:
        PYTHONPATH: .

  # Fast tests for development workflow
  fast-tests:
    name: ‚ö° Fast Tests
    runs-on: ubuntu-latest
    if: github.event.inputs.test_type == 'fast' || github.event_name == 'pull_request'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python 3.12
      uses: actions/setup-python@v5
      with:
        python-version: '3.12'

    - name: Cache dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-fast-${{ hashFiles('**/requirements*.txt') }}

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install pytest pytest-asyncio pytest-mock
        pip install -r requirements.txt

    - name: Run fast tests
      run: |
        python -m pytest -m "not slow and not external" tests/ --tb=short --no-cov -v
      env:
        PYTHONPATH: .

  # Comprehensive unit tests
  unit-tests:
    name: üîß Unit Tests
    runs-on: ubuntu-latest
    if: github.event.inputs.test_type == 'unit' || github.event.inputs.test_type == 'all' || github.event_name == 'push'
    strategy:
      matrix:
        python-version: ['3.11', '3.12']

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v5
      with:
        python-version: ${{ matrix.python-version }}

    - name: Cache dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ matrix.python-version }}-${{ hashFiles('**/requirements*.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-${{ matrix.python-version }}-

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest-cov pytest-html pytest-json-report

    - name: Create test directories
      run: |
        mkdir -p tests/logs
        mkdir -p test_reports

    - name: Run unit tests
      run: |
        python -m pytest tests/unit/ tests/test_basic_setup.py -m "unit or not integration" --tb=short -v \
          --cov=crawler --cov=monitoring --cov=clients --cov=utils --cov=models \
          --cov-report=xml --cov-report=html \
          --junit-xml=test_reports/junit-unit-${{ matrix.python-version }}.xml
      env:
        PYTHONPATH: .
        QDRANT_URL: http://localhost:6333
        QDRANT_API_KEY: test-api-key
        OPENAI_API_KEY: test-openai-key
        AZURE_OPENAI_DEPLOYMENT: test-deployment
        ALERT_SLACK_ENABLED: false

    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        flags: unittests
        name: codecov-unit-${{ matrix.python-version }}
        fail_ci_if_error: false

    - name: Upload test artifacts
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: unit-test-results-${{ matrix.python-version }}
        path: |
          htmlcov/
          coverage.xml
          test_reports/
          tests/logs/

  # Integration tests
  integration-tests:
    name: üîó Integration Tests
    runs-on: ubuntu-latest
    if: github.event.inputs.test_type == 'integration' || github.event.inputs.test_type == 'all' || github.event_name == 'push'
    needs: smoke-tests

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python 3.12
      uses: actions/setup-python@v5
      with:
        python-version: '3.12'

    - name: Cache dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-integration-${{ hashFiles('**/requirements*.txt') }}

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest-html

    - name: Run integration tests (no external)
      run: |
        python -m pytest tests/integration/ -m "integration and not external" --tb=short -v \
          --html=test_reports/integration-report.html --self-contained-html \
          --maxfail=10
      env:
        PYTHONPATH: .
        QDRANT_URL: http://localhost:6333
        QDRANT_API_KEY: test-api-key
        OPENAI_API_KEY: test-openai-key
        ALERT_SLACK_ENABLED: false
      continue-on-error: true

    - name: Upload integration test results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: integration-test-results
        path: test_reports/

  # LLM functionality tests (optional)
  llm-tests:
    name: üß† LLM Tests
    runs-on: ubuntu-latest
    if: github.event.inputs.test_type == 'all'
    continue-on-error: true
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python 3.12
      uses: actions/setup-python@v5
      with:
        python-version: '3.12'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt

    - name: Run LLM tests
      run: |
        python -m pytest tests/llm/ -m llm --tb=short -v || echo "LLM tests may be skipped if API keys not configured"
      env:
        PYTHONPATH: .
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY || 'test-key' }}
        AZURE_OPENAI_DEPLOYMENT: test-deployment

  # Code quality checks
  quality-checks:
    name: üîç Quality Checks
    runs-on: ubuntu-latest
    continue-on-error: true
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python 3.12
      uses: actions/setup-python@v5
      with:
        python-version: '3.12'
    
    - name: Install quality tools
      run: |
        pip install flake8 bandit[toml] safety
    
    - name: Run linting
      run: |
        flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics || echo "Linting completed"
      continue-on-error: true
    
    - name: Security scan
      run: |
        bandit -r . -ll -x tests/ || echo "Security scan completed"
      continue-on-error: true
    
    - name: Dependency security check
      run: |
        safety check || echo "Dependency check completed"
      continue-on-error: true

  # Test completion gate
  test-gate:
    name: ‚úÖ Test Gate
    runs-on: ubuntu-latest
    needs: [smoke-tests, unit-tests]
    if: always() && needs.smoke-tests.result == 'success'
    
    steps:
    - name: Test results summary
      run: |
        echo "üìä Test Pipeline Results:"
        echo "========================"
        echo "üî• Smoke Tests: ${{ needs.smoke-tests.result }}"
        echo "üîß Unit Tests: ${{ needs.unit-tests.result }}"
        echo "üîó Integration Tests: ${{ needs.integration-tests.result || 'skipped' }}"
        echo ""
        if [ "${{ needs.smoke-tests.result }}" = "success" ] && [ "${{ needs.unit-tests.result }}" = "success" ]; then
          echo "‚úÖ Core tests passed - Ready for deployment!"
          exit 0
        else
          echo "‚ùå Some tests failed - Review before deploying"
          exit 1
        fi
